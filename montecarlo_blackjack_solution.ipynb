{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea369429",
   "metadata": {},
   "source": [
    "\n",
    "# Monte Carlo Control for Blackjack (`Blackjack-v1`, Gymnasium)\n",
    "\n",
    "Este notebook implementa un agente que aprende a jugar Blackjack usando **control por Monte Carlo on‑policy** con una política **ε-greedy**. Pasos:\n",
    "\n",
    "1. Política inicial aleatoria (ε-greedy sobre `Q` inicial).\n",
    "2. Simulación de episodios completos desde el inicio hasta un estado terminal.\n",
    "3. Estimación de `Q(s, a)` con **first-visit MC**.\n",
    "4. Mejora de política usando `argmax_a Q(s,a)` dentro de un esquema ε-greedy.\n",
    "5. Medición del rendimiento promedio de la política final.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Requisitos\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\n",
    "        \"Gymnasium no está instalado en este entorno. \"\n",
    "        \"Instale con: pip install gymnasium[classic-control]\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear entorno Blackjack-v1\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)  # sab=True usa la versión compatible con Sutton & Barto\n",
    "nA = 2  # acciones: 0=Stick, 1=Hit\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "env.reset(seed=seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ee8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_action(Q, state, nA=2, epsilon=0.1, rng=None):\n",
    "    \"\"\"Devuelve una acción ε-greedy dada Q y el estado.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    if rng.random() < epsilon:\n",
    "        return rng.integers(nA)\n",
    "    else:\n",
    "        return int(np.argmax(Q[state]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_episode(env, Q, epsilon=0.1, rng=None):\n",
    "    \"\"\"Genera un episodio completo (s, a, r) usando política ε-greedy sobre Q.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    episode = []\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(Q, state, nA=2, epsilon=epsilon, rng=rng)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    return episode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mc_control_on_policy(num_episodes=50000, gamma=1.0, epsilon=0.1, seed=42, log_every=10000):\n",
    "    \"\"\"\n",
    "    Control por Monte Carlo (first-visit) on-policy con ε-greedy.\n",
    "    Devuelve: Q (defaultdict), política derivada (π*), historial de retornos medios.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Q = defaultdict(lambda: np.zeros(2, dtype=float))\n",
    "    returns_sum = defaultdict(lambda: np.zeros(2, dtype=float))\n",
    "    returns_count = defaultdict(lambda: np.zeros(2, dtype=float))\n",
    "\n",
    "    average_returns = []\n",
    "    window = 2000\n",
    "    recent_g = []\n",
    "\n",
    "    for i in range(1, num_episodes + 1):\n",
    "        episode = generate_episode(env, Q, epsilon=epsilon, rng=rng)\n",
    "\n",
    "        # First-visit: registrar primeras ocurrencias por (s,a)\n",
    "        sa_first_visit = set()\n",
    "        G = 0.0\n",
    "        # Retropropagar retornos\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "            if (s, a) not in sa_first_visit:\n",
    "                sa_first_visit.add((s, a))\n",
    "                returns_sum[s][a] += G\n",
    "                returns_count[s][a] += 1.0\n",
    "                Q[s][a] = returns_sum[s][a] / max(1.0, returns_count[s][a])\n",
    "\n",
    "        # Seguimiento de retorno del episodio para monitoreo\n",
    "        episode_return = sum(r for (_, _, r) in episode)\n",
    "        recent_g.append(episode_return)\n",
    "        if len(recent_g) > window:\n",
    "            recent_g.pop(0)\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            avg = float(np.mean(recent_g)) if recent_g else 0.0\n",
    "            average_returns.append((i, avg))\n",
    "            print(f\"Episodio {i}: retorno promedio (últimos {window}) = {avg:.4f}\")\n",
    "\n",
    "    # Política final determinista (greedy)\n",
    "    def greedy_pi(state):\n",
    "        return int(np.argmax(Q[state]))\n",
    "\n",
    "    return Q, greedy_pi, np.array(average_returns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8facfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entrenamiento (ajuste el número de episodios según su tiempo disponible)\n",
    "NUM_EPISODES = 50000\n",
    "EPSILON = 0.1\n",
    "GAMMA = 1.0\n",
    "\n",
    "Q, greedy_pi, monitor = mc_control_on_policy(num_episodes=NUM_EPISODES, gamma=GAMMA, epsilon=EPSILON, seed=seed, log_every=10000)\n",
    "print(\"Entrenamiento completado.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluación de la política aprendida\n",
    "def evaluate_policy(env, policy_fn, episodes=10000, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    total_return = 0.0\n",
    "    for _ in range(episodes):\n",
    "        s, info = env.reset(seed=int(rng.integers(0, 10_000_000)))\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        while not done:\n",
    "            a = policy_fn(s)\n",
    "            s, r, terminated, truncated, info = env.step(a)\n",
    "            G += r\n",
    "            done = terminated or truncated\n",
    "        total_return += G\n",
    "    avg_return = total_return / episodes\n",
    "    return avg_return\n",
    "\n",
    "avg_return = evaluate_policy(env, greedy_pi, episodes=20000, seed=999)\n",
    "print(f\"Rendimiento promedio de la política final: {avg_return:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualización básica (una figura por gráfico como se solicita)\n",
    "# 1) Curva de aprendizaje: retorno promedio (ventana deslizante)\n",
    "if monitor.size > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(monitor[:,0], monitor[:,1])\n",
    "    plt.title(\"Retorno promedio (ventana) durante el entrenamiento\")\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Retorno promedio\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Sin datos de monitoreo suficientes para graficar.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ddf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Mapa de políticas: para cada par (player sum, dealer showing), con o sin usable ace\n",
    "# Estados: (player_sum in [12..21], dealer_showing in [1..10], usable_ace in {False, True})\n",
    "import pandas as pd\n",
    "\n",
    "def build_policy_grid(Q, usable_ace=False):\n",
    "    rows = []\n",
    "    for ps in range(12, 22):\n",
    "        row = []\n",
    "        for dealer in range(1, 11):\n",
    "            s = (ps, dealer, usable_ace)\n",
    "            a = int(np.argmax(Q[s])) if s in Q else 1  # por defecto: hit\n",
    "            row.append(a)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows, index=list(range(12,22)), columns=list(range(1,11)))\n",
    "    return df\n",
    "\n",
    "pi_no_ace = build_policy_grid(Q, usable_ace=False)\n",
    "pi_ace    = build_policy_grid(Q, usable_ace=True)\n",
    "\n",
    "# Mostrar como tablas (0=Stick, 1=Hit)\n",
    "from ace_tools import display_dataframe_to_user\n",
    "display_dataframe_to_user(\"Política (usable_ace=False) 0=Stick,1=Hit\", pi_no_ace)\n",
    "display_dataframe_to_user(\"Política (usable_ace=True)  0=Stick,1=Hit\", pi_ace)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c45c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Heatmap de V(s) = max_a Q(s,a) para usable_ace=False/True\n",
    "def build_value_grid(Q, usable_ace=False):\n",
    "    grid = np.zeros((10,10))  # player_sum: 12..21 (10 valores), dealer: 1..10 (10 valores)\n",
    "    for i, ps in enumerate(range(12,22)):\n",
    "        for j, dealer in enumerate(range(1,11)):\n",
    "            s = (ps, dealer, usable_ace)\n",
    "            if s in Q:\n",
    "                grid[i,j] = np.max(Q[s])\n",
    "            else:\n",
    "                grid[i,j] = 0.0\n",
    "    return grid\n",
    "\n",
    "V_no_ace = build_value_grid(Q, usable_ace=False)\n",
    "V_ace    = build_value_grid(Q, usable_ace=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(V_no_ace, origin=\"lower\", extent=[1,10,12,21], aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.title(\"V(s)=max_a Q(s,a), usable_ace=False\")\n",
    "plt.xlabel(\"Dealer showing\")\n",
    "plt.ylabel(\"Player sum\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(V_ace, origin=\"lower\", extent=[1,10,12,21], aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.title(\"V(s)=max_a Q(s,a), usable_ace=True\")\n",
    "plt.xlabel(\"Dealer showing\")\n",
    "plt.ylabel(\"Player sum\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61250d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Guardar objetos y utilidades\n",
    "import pickle, os\n",
    "\n",
    "out_dir = \"/mnt/data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(out_dir, \"Q_blackjack_mc.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(dict(Q), f)\n",
    "\n",
    "# Guardar la política como función discreta (tabla)\n",
    "pi_table = {k: int(np.argmax(v)) for k, v in Q.items()}\n",
    "with open(os.path.join(out_dir, \"policy_greedy_blackjack.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(pi_table, f)\n",
    "\n",
    "print(\"Archivos guardados en:\", out_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f19160",
   "metadata": {},
   "source": [
    "\n",
    "## Cómo usar/entregar\n",
    "\n",
    "- El corazón del control por Monte Carlo está en `mc_control_on_policy` (first-visit, ε-greedy).\n",
    "- Para medir rendimiento, use `evaluate_policy`. Puede ajustar el número de episodios.\n",
    "- Para su informe, incluya:\n",
    "  - Descripción del método (MC first‑visit on‑policy, ε-greedy).\n",
    "  - Hiperparámetros: número de episodios, ε, γ, semilla.\n",
    "  - Curva de aprendizaje y mapas de política/valor (opcional pero recomendado).\n",
    "  - Promedio de retorno de la política final.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
